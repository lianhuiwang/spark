package org.apache.spark.sql.execution.adaptive

import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.catalyst.expressions.SortOrder
import org.apache.spark.sql.catalyst.plans.physical._
import org.apache.spark.sql.catalyst.rules.Rule
import org.apache.spark.sql.execution.SparkPlan
import org.apache.spark.sql.execution.exchange.ShuffleExchange

/**
 * Transform a physical plan tree into an query fragment tree
 */
case class QueryFragmentTransformer(sqlContext: SQLContext,maxIterations: Int = 100)
  extends Rule[SparkPlan] {

  def apply(plan: SparkPlan): SparkPlan = {
    val newPlan = plan.transformUp {
      case operator: SparkPlan => withQueryFragment(operator)
    }
    val childFragments = Utils.findChildFragment(newPlan)
    val newFragment = new QueryFragment(childFragments, true)
    newFragment.setRootPlan(newPlan)
    newFragment
  }

  private[this] def withQueryFragment(operator: SparkPlan): SparkPlan = {
    val requiredChildDistributions: Seq[Distribution] = operator.requiredChildDistribution
    val requiredChildOrderings: Seq[Seq[SortOrder]] = operator.requiredChildOrdering
    val children: Seq[SparkPlan] = operator.children
    assert(requiredChildDistributions.length == children.length)
    assert(requiredChildOrderings.length == children.length)

    val supportsAdaptiveExecution =
      if (children.exists(_.isInstanceOf[ShuffleExchange])) {
        // Right now, Adaptive execution only support HashPartitionings.
        children.forall {
          case e @ ShuffleExchange(hash: HashPartitioning, _, _) => true
          case child =>
            child.outputPartitioning match {
              case hash: HashPartitioning => true
              case collection: PartitioningCollection =>
                collection.partitionings.forall(_.isInstanceOf[HashPartitioning])
              case _ => false
            }
        }
      } else {
        // In this case, although we do not have Exchange operators, we may still need to
        // shuffle data when we have more than one children because data generated by
        // these children may not be partitioned in the same way.
        // Please see the comment in withCoordinator for more details.
        val supportsDistribution =
          requiredChildDistributions.forall(_.isInstanceOf[ClusteredDistribution])
        children.length > 1 && supportsDistribution
      }

    val withFragments =
      if (supportsAdaptiveExecution) {
        children.zip(requiredChildDistributions).map {
          case (e: ShuffleExchange, _) =>
            // This child is an Exchange, we need to add the fragment.
            val childFragments = Utils.findChildFragment(e)
            val newFragment = new QueryFragment(childFragments, false)
            val fragmentInput = FragmentInput(newFragment)
            fragmentInput.setInputPlan(e)
            newFragment.setExchange(e)
            newFragment.setFragmentInput(fragmentInput)
            fragmentInput

          case (child, distribution) =>
            child
        }
      } else {
        children
      }

    operator.withNewChildren(withFragments)
  }
}
